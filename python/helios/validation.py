from helios.utils import find_file, find_files, get_asset_directories, is_real_iterable

from collections.abc import Iterable, Mapping
from pathlib import Path
from pint import UnitRegistry
from pydantic import validate_call, GetCoreSchemaHandler
from pydantic.functional_validators import AfterValidator, BeforeValidator
from pydantic_core import core_schema
from typing import Any, Literal, Optional, Type, Union, get_origin, get_args
from typing_extensions import Annotated, dataclass_transform
from numpydantic import NDArray, Shape

import annotated_types
import inspect
import multiprocessing
import numpy as np
import os
import xmlschema
from datetime import date, datetime, time
from enum import Enum

from copy import deepcopy

# The global registry of physical units used in Helios++. Currently there
# are no custom units, so we go with Pint's default registry.
units = UnitRegistry()


def _unit_validator(default_unit):
    """A validator function that converts to a unit if necessary"""

    def _validator(v):
        quantity = units.Quantity(v)
        if quantity.unitless:
            return v
        return quantity.to(default_unit).magnitude

    return BeforeValidator(_validator)


Angle = Annotated[float, _unit_validator(units.rad)]
AngleVelocity = Annotated[float, _unit_validator(units.rad / units.s)]
Frequency = Annotated[int, _unit_validator(units.Hz), annotated_types.Ge(0)]
# NB: The reason we are not reusing the Frequency annotation here is because
#     the backend uses doubles for scan frequency, but ints for other frequencies.
#     Unifying them will either lead to loss of precision or to (correct) warnings
#     during serialization. This can be fixed by changing types in the backend, but
#     for the moment we stick with a separate annotation.
ScanFrequency = Annotated[float, _unit_validator(units.Hz), annotated_types.Ge(0)]
Length = Annotated[float, _unit_validator(units.m), annotated_types.Ge(0)]
TimeInterval = Annotated[float, _unit_validator(units.s), annotated_types.Ge(0)]


def _coerce_vector(length: int, name: str):
    def _validator(value: Any):
        if value is None:
            return value
        if isinstance(value, (list, tuple)) and len(value) != length:
            raise ValueError(f"Value must be a {name} of length {length}.")
        try:
            return np.asarray(value, dtype=np.float64)
        except (TypeError, ValueError) as exc:
            raise ValueError(f"Value must be a {name} coercible to float64.") from exc

    return _validator


R3Vector = Annotated[
    NDArray[Shape["3"], np.float64],
    BeforeValidator(_coerce_vector(3, "3D vector")),
]

Quaternion = Annotated[
    NDArray[Shape["4"], np.float64],
    BeforeValidator(_coerce_vector(4, "quaternion")),
]


def validate_xml_file(file_path: Path, schema_path: Path):
    """Validate an XML file against an XML schema"""

    # Resolve file paths
    file_path = find_file(file_path)
    schema_path = find_file(schema_path)

    # Validate the XML file against the schema
    schema = xmlschema.XMLSchema(str(schema_path))
    schema.validate(str(file_path))


def _validate_thread_count(count: Union[int, None]) -> int:
    physical = multiprocessing.cpu_count()
    if count is None:
        return physical

    if count < 1:
        raise ValueError(
            "Thread count must be greater than 0, use None for automatic detection"
        )
    if count > physical:
        raise ValueError(
            f"Thread count must be less than or equal to the available number of cores ({physical})"
        )

    return count


def _create_directory(directory: Path):
    os.makedirs(directory, exist_ok=True)
    return directory


# Some type annotations for convenience
AssetPath = Annotated[Path, AfterValidator(find_file)]
MultiAssetPath = Annotated[list[Path], BeforeValidator(find_files)]
ThreadCount = Annotated[Optional[int], AfterValidator(_validate_thread_count)]
CreatedDirectory = Annotated[Path, AfterValidator(_create_directory)]


def _coerce_compression_level(level: Any) -> int:
    if isinstance(level, str):
        level_l = level.lower()
        if level_l == "none":
            return 0
        if level_l == "default":
            return 6
        if level_l == "fast":
            return 1
        if level_l == "best":
            return 9
        raise ValueError(
            "Compression level must be an int in [0, 9], 'none', 'default', "
            "'fast' or 'best'."
        )
    if isinstance(level, bool):
        raise ValueError(
            "Compression level must be an int in [0, 9], 'none', 'default', "
            "'fast' or 'best'."
        )
    if isinstance(level, (int, np.integer)):
        return int(level)
    raise ValueError(
        "Compression level must be an int in [0, 9], 'none', 'default', "
        "'fast' or 'best'."
    )


CompressionLevel = Annotated[
    Union[int, Literal["none", "default", "fast", "best"]],
    BeforeValidator(_coerce_compression_level),
    annotated_types.Ge(0),
    annotated_types.Le(9),
]


def _is_iterable_annotation(a):
    """Determine whether a type annotation describes an iterable."""

    origin = get_origin(a)
    if origin is None:
        return False
    if not isinstance(origin, type):
        return False
    return issubclass(origin, Iterable)


def _is_iterable_of_model_annotation(a):
    """Determine whether this type annotation describes an iterable of Models"""

    if not _is_iterable_annotation(a):
        return False

    args = get_args(a)
    if len(args) == 0 or not isinstance(args[0], type):
        return False
    return issubclass(args[0], Model)


def _is_optional(t: Type) -> bool:
    origin = get_origin(t)
    return origin is Union and type(None) in get_args(t)


def _inner_optional_type(t: Type) -> Type:
    return next(arg for arg in get_args(t) if arg is not type(None))


def get_all_annotations(cls):
    """Collect annotations from cls and all its bases (excluding `object`)."""
    anns: dict[str, type] = {}
    for base in reversed(cls.__mro__):
        if base is object:
            continue
        anns.update(inspect.get_annotations(base))
    return anns


def get_all_defaults(cls, dct):
    """Collect default values for fields from cls and all its bases (excluding `object`)."""
    defaults = {}
    for base in reversed(cls.mro()[:-1]):
        if base is object:
            continue
        base_defaults = getattr(base, "_defaults", None)
        if isinstance(base_defaults, dict):
            defaults.update(base_defaults)

    for field in inspect.get_annotations(cls).keys():
        if field in dct:
            val = dct[field]
            if not isinstance(val, property):
                defaults[field] = val

    return defaults


def _normalize_provenance_value(value: Any):
    """Convert runtime values into YAML-safe primitives."""

    if isinstance(value, (str, int, float, bool)) or value is None:
        return value

    if isinstance(value, Path):
        path = value.expanduser()
        if not path.is_absolute():
            return str(path)

        asset_dirs = [
            Path(directory).expanduser().resolve()
            for directory in get_asset_directories()
        ]
        relative_candidates = []
        resolved_path = path.resolve()
        for asset_dir in asset_dirs:
            try:
                relative_candidates.append(resolved_path.relative_to(asset_dir))
            except ValueError:
                continue

        if relative_candidates:
            shortest = min(
                relative_candidates,
                key=lambda candidate: (len(candidate.parts), len(str(candidate))),
            )
            return str(shortest)

        if asset_dirs:
            try:
                return os.path.relpath(resolved_path, start=asset_dirs[0])
            except ValueError:
                return str(resolved_path)

        try:
            return os.path.relpath(resolved_path, start=Path.cwd())
        except ValueError:
            return str(resolved_path)

    if isinstance(value, Enum):
        return value.value

    if isinstance(value, (datetime, date, time)):
        return value.isoformat()

    if isinstance(value, np.ndarray):
        return value.tolist()

    if isinstance(value, np.generic):
        return value.item()

    if isinstance(value, Mapping):
        return {
            str(key): _normalize_provenance_value(item) for key, item in value.items()
        }

    if is_real_iterable(value):
        return [_normalize_provenance_value(item) for item in value]

    return str(value)


def _iter_models_in_value(value: Any):
    if isinstance(value, Model):
        yield value
        return

    if isinstance(value, Mapping):
        for item in value.values():
            yield from _iter_models_in_value(item)
        return

    if isinstance(value, np.ndarray):
        return

    if is_real_iterable(value):
        for item in value:
            yield from _iter_models_in_value(item)


@dataclass_transform()
class ValidatedModelMetaClass(type):
    def __new__(cls, name, bases, dct, **kwargs):
        cls = super().__new__(cls, name, bases, dct, **kwargs)

        annotations = get_all_annotations(cls)
        # Determine fields that should be properties
        defaults = get_all_defaults(cls, dct)
        setattr(cls, "_defaults", defaults)

        # Creation of properties need to be wrapped in a function to accommodate
        # a semantic weirdness of Python: If you define a function inside a loop,
        # the function will capture the loop variable, not the value of the loop
        # variable at the time of the function definition. Wrapping the logic in
        # a function will create a new scope for the loop variable, which will
        # then be captured correctly by the function.
        def _make_property(f, a):
            def _getter(self):
                if _is_optional(a) and hasattr(self, f"_{f}"):
                    return getattr(self, f"_{f}")

                # If the property is backed by a C++ object, we first check for
                # potential updates for the Python object
                if hasattr(self, "_cpp_object") and hasattr(self._cpp_object, f):
                    value = getattr(self._cpp_object, f)

                    if _is_optional(a):
                        T = _inner_optional_type(a)
                        if value is None:
                            wrapped = None
                        elif hasattr(T, "_from_cpp"):
                            wrapped = T._from_cpp(value)
                        else:
                            wrapped = value
                        setattr(self, f"_{f}", wrapped)
                        return wrapped

                    # Determine whether this is of type Iterable[Model]
                    if _is_iterable_of_model_annotation(a):

                        def _check_if_rebuild():
                            if len(value) != len(getattr(self, f"_{f}")):
                                return True
                            for i, val in enumerate(value):
                                if val is not getattr(self, f"_{f}")[i]._cpp_object:
                                    return True
                            return False

                        if _check_if_rebuild():
                            setattr(
                                self,
                                f"_{f}",
                                [get_args(a)[0]._from_cpp(val) for val in value],
                            )
                    else:
                        if hasattr(a, "_cpp_class"):
                            if not hasattr(self, f"_{f}"):
                                if hasattr(a, "_from_cpp"):
                                    setattr(self, f"_{f}", a._from_cpp(value))
                                    return getattr(self, f"_{f}")
                                setattr(self, f"_{f}", value)
                                return value
                            getattr(self, f"_{f}")._cpp_object = value
                            return getattr(self, f"_{f}")
                        return value

                # Otherwise, we take the property from a variable with a prefixed underscore
                return getattr(self, f"_{f}")

            @validate_call
            def _setter(self, value: a):
                # If a pre_set hook was provided, we execute it
                self._pre_set(f, value)

                # We always store the object in a variable with a prefixed underscore
                setattr(self, f"_{f}", value)

                # If this property is backed by a C++ object, we additionally set it on C++
                if hasattr(self, "_cpp_object") and hasattr(self._cpp_object, f):
                    if (
                        is_real_iterable(value)
                        and len(value) > 0
                        and hasattr(value[0], "_cpp_object")
                    ):
                        value = [v._cpp_object for v in value]
                    if hasattr(value, "_cpp_object"):
                        value = value._cpp_object
                    setattr(self._cpp_object, f, value)

                # If a post_set hook was provided, we execute it
                self._post_set(f)

            # Register the getter and setter on the property
            return property().getter(_getter).setter(_setter)

        # Make fields properties
        for field, annot in annotations.items():
            setattr(cls, field, _make_property(field, annot))

        # Create an __init__ for the class
        def __init__(self, *args, **instance_kwargs):

            self._during_init = True

            # Iterate the fields in exactly the given order. When using a different order,
            # we risk that properties are instantiated in an incorrect order.
            for i, field in enumerate(annotations):
                # Check whether we find this among positional arguments
                if i < len(args):
                    setattr(self, field, args[i])
                    continue

                # Check whether we find this among keyword arguments
                if field in instance_kwargs:
                    setattr(self, field, instance_kwargs[field])
                    continue

                if field in cls._defaults:
                    default_value = cls._defaults[field]
                    if default_value is None and _is_optional(annotations[field]):
                        setattr(self, field, None)
                        continue

                    # Make a deepcopy if it's a known mutable type
                    if isinstance(default_value, (list, dict, set)) or hasattr(
                        default_value, "__deepcopy__"
                    ):
                        default_value = deepcopy(default_value)

                    setattr(self, field, default_value)
                    continue

                if _is_optional(annotations[field]):
                    if hasattr(_inner_optional_type(annotations[field]), "_cpp_class"):
                        setattr(self, field, None)
                        continue

                # Raise an error if this was required and not we reached this point
                raise ValueError(f"Missing required argument: {field}")

            self._during_init = False
            instance_kwargs.pop("_cpp_object", None)
            invalid_fields = set(instance_kwargs) - set(annotations)
            if invalid_fields:
                raise ValueError(f"Invalid fields passed: {', '.join(invalid_fields)}")

        setattr(cls, "__init__", __init__)

        return cls


class Model(metaclass=ValidatedModelMetaClass):
    """Base class for validated objects in Helios++."""

    def __init_subclass__(cls, cpp_class=None, **kwargs):
        super().__init_subclass__(**kwargs)
        if not hasattr(cls, "_cpp_class"):
            cls._cpp_class = None
        if cpp_class is not None:
            cls._cpp_class = cpp_class

    def __new__(cls, *args, **kwargs):
        obj = super().__new__(cls)
        obj._yaml_serializable = True
        if cls._cpp_class is not None:
            cpp_object = kwargs.pop("_cpp_object", None)
            if cpp_object is None:
                cpp_object = cls._cpp_class()
            obj._cpp_object = cpp_object
        return obj

    def _post_set(self, field):
        """Hook that is called after a property is set"""
        pass

    def _pre_set(self, field, value):
        """Hook that is called before a property is set"""
        pass

    def _serialization_filename(self) -> str:
        """Hook for naming YAML files in shallow serialization."""

        return f"{self.__class__.__name__.lower()}.yaml"

    def _serialization_binary_filename(self) -> str:
        """Hook for naming binary sidecar files."""

        return f"{Path(self._serialization_filename()).stem}.bin"

    @validate_call
    def to_yaml(self, path: Path, shallow: bool = True, binary: bool = False):
        from helios.serialization import serialize_model_to_yaml

        if not getattr(self, "_yaml_serializable", True):
            raise RuntimeError(
                f"{self.__class__.__name__} cannot be serialized to YAML because it "
                "was implicitly constructed via a parent object's from_xml constructor."
            )

        return serialize_model_to_yaml(
            self, path=path.expanduser(), shallow=shallow, binary=binary
        )

    @validate_call
    def to_bundle(self, path: Path, binary: bool = False, force: bool = False):
        from helios.serialization import serialize_model_to_bundle

        return serialize_model_to_bundle(
            self, path=path.expanduser(), binary=binary, force=force
        )

    @classmethod
    @validate_call
    def from_yaml(cls, path: Path):
        from helios.serialization import deserialize_model_from_yaml

        return deserialize_model_from_yaml(cls, path=path.expanduser())

    @classmethod
    def __get_pydantic_core_schema__(
        cls, source: Type[Any], handler: GetCoreSchemaHandler
    ):
        return core_schema.no_info_after_validator_function(
            cls._validate, core_schema.any_schema()
        )

    @classmethod
    def _validate(cls, value):
        if not isinstance(value, cls):
            raise ValueError(f"Expected {cls.__name__}, got {type(value).__name__}")
        return value

    def __repr__(self):
        if hasattr(self, "_cpp_object"):
            return f"<{self.__class__.__name__} at {hex(id(self._cpp_object))}>"
        else:
            return f"<{self.__class__.__name__} at {hex(id(self))}>"

    @classmethod
    def _from_cpp(cls, value):
        params = {}
        for field, annot in get_all_annotations(cls).items():
            if hasattr(value, field):
                cpp_value = getattr(value, field)
                if _is_optional(annot):
                    T = _inner_optional_type(annot)
                    params[field] = (
                        None if cpp_value is None else T._from_cpp(cpp_value)
                    )
                    continue

                if not _is_iterable_annotation(annot):
                    if hasattr(annot, "_from_cpp"):
                        cpp_value = annot._from_cpp(cpp_value)
                else:
                    args = get_args(annot)
                    if hasattr(args[0], "_from_cpp"):
                        cpp_value = [args[0]._from_cpp(v) for v in cpp_value]
                params[field] = cpp_value

        return cls(_cpp_object=value, **params)

    def _enforce_uniqueness_across_instances(self, field, value):
        """Enforce uniqueness of a field across all instances of a class"""

        # Ensure that the uniqueness dictionary is present
        if not hasattr(self.__class__, f"_uniqueness_{field}"):
            setattr(self.__class__, f"_uniqueness_{field}", {})

        # Extract the information mapping values to instances
        info = getattr(self.__class__, f"_uniqueness_{field}")

        # If the value already exists on another instance, raise an error
        def _check_if_exists(val):
            if val in info.keys() and not info[val] is self:
                raise ValueError(f"Value {val} is already used by another instance")

        # For iterables, we need to check every single value
        if is_real_iterable(value):
            for val in value:
                _check_if_exists(val)
                info[val] = self
        else:
            _check_if_exists(value)
            info[value] = self

    def _set_yaml_serializable(self, value: bool, recursive: bool = False, _seen=None):
        self._yaml_serializable = value

        if not recursive:
            return

        if _seen is None:
            _seen = set()

        obj_id = id(self)
        if obj_id in _seen:
            return
        _seen.add(obj_id)

        for field in get_all_annotations(self.__class__):
            for model in _iter_models_in_value(getattr(self, field)):
                model._set_yaml_serializable(value, recursive=True, _seen=_seen)

    def _disable_yaml_serialization_for_descendants(self, _seen=None):
        if _seen is None:
            _seen = set()

        obj_id = id(self)
        if obj_id in _seen:
            return
        _seen.add(obj_id)

        for field in get_all_annotations(self.__class__):
            for model in _iter_models_in_value(getattr(self, field)):
                model._set_yaml_serializable(False, recursive=True, _seen=_seen)

    def _set_constructor_provenance(self, method: str, **kwargs):
        self._provenance = {
            "constructor": {
                "method": method,
                "kwargs": {
                    key: _normalize_provenance_value(value)
                    for key, value in kwargs.items()
                },
            },
            "operations": [],
        }

    def _append_operation_provenance(self, method: str, **kwargs):
        provenance = getattr(self, "_provenance", None)
        if not isinstance(provenance, dict):
            provenance = {}

        operations = provenance.setdefault("operations", [])
        operations.append(
            {
                "method": method,
                "kwargs": {
                    key: _normalize_provenance_value(value)
                    for key, value in kwargs.items()
                    if value is not None
                },
            }
        )

        self._provenance = provenance

    def _reconstruct_from_model_fields(self, memo):
        cls = self.__class__
        obj_id = id(self)
        if obj_id in memo:
            return memo[obj_id]

        annotations = get_all_annotations(cls)

        # Allocate and memoize before deepcopying fields so recursive references resolve.
        cpp_copy = None
        if cls._cpp_class is not None and hasattr(self, "_cpp_object"):
            if hasattr(self._cpp_object, "clone"):
                cpp_copy = self._cpp_object.clone()
            else:
                try:
                    cpp_copy = cls._cpp_class(self._cpp_object)
                except TypeError:
                    pass

        if cpp_copy is not None:
            reconstructed = cls.__new__(cls, _cpp_object=cpp_copy)
            memo[obj_id] = reconstructed

            # Keep C++-backed fields from the cloned C++ object graph.
            # For scalar C++-backed fields, preserve Python-level values.
            # For Model-typed C++-backed fields, keep the C++ clone wiring.
            reconstructed._during_init = True
            try:
                for field, annot in annotations.items():
                    is_iterable_model = _is_iterable_of_model_annotation(annot)
                    is_model = False
                    if _is_optional(annot):
                        inner = _inner_optional_type(annot)
                        is_model = isinstance(inner, type) and hasattr(
                            inner, "_cpp_class"
                        )
                    else:
                        is_model = isinstance(annot, type) and hasattr(
                            annot, "_cpp_class"
                        )

                    if hasattr(cpp_copy, field) and (is_model or is_iterable_model):
                        # Iterable[Model] getters expect an initialized cache.
                        if is_iterable_model and not hasattr(
                            reconstructed, f"_{field}"
                        ):
                            setattr(reconstructed, f"_{field}", [])
                        # Prime wrapper cache from C++ clone graph.
                        getattr(reconstructed, field)
                        continue
                    setattr(reconstructed, field, deepcopy(getattr(self, field), memo))
            finally:
                reconstructed._during_init = False

            if hasattr(self, "_provenance"):
                reconstructed._provenance = deepcopy(self._provenance, memo)
            if hasattr(self, "_yaml_serializable"):
                reconstructed._yaml_serializable = self._yaml_serializable

            return reconstructed
        else:
            reconstructed = cls.__new__(cls)
            memo[obj_id] = reconstructed

            kwargs = {}
            for field in annotations:
                kwargs[field] = deepcopy(getattr(self, field), memo)

            reconstructed.__init__(**kwargs)

            if hasattr(self, "_provenance"):
                reconstructed._provenance = deepcopy(self._provenance, memo)
            if hasattr(self, "_yaml_serializable"):
                reconstructed._yaml_serializable = self._yaml_serializable

            return reconstructed

    def clone(self):
        """Create a deep copy of the object"""
        return self._reconstruct_from_model_fields({})

    def __deepcopy__(self, memo):
        """Create a deep copy of the object using model field reconstruction."""
        return self._reconstruct_from_model_fields(memo)


class UpdateableMixin:
    """Mixin for objects that can be updated from another object"""

    def update_from_dict(self, kwargs, skip_exceptions=False):
        """Update a ValidatedCppModel object from a dictionary of keyword arguments"""

        keys = list(kwargs.keys())

        anns = get_all_annotations(self.__class__)
        for key in keys:
            cls_attr = self.__class__.__dict__.get(key, None)
            if key in anns:
                setattr(self, key, kwargs.pop(key))
            elif isinstance(cls_attr, property) and cls_attr.fset is not None:
                setattr(self, key, kwargs.pop(key))
            elif not skip_exceptions:
                raise ValueError(f"Invalid key: {key}")

    def update_from_object(self, other, skip_exceptions=False):
        """Update a ValidatedCppModel object from another object"""

        if not isinstance(self, Model):
            raise ValueError("update_from_object can only be called on Model objects")

        parameters = {}
        for field in get_all_annotations(self.__class__).keys():
            parameters[field] = getattr(other, field)
        self.update_from_dict(parameters, skip_exceptions=skip_exceptions)
